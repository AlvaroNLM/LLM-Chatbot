{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8d916e",
   "metadata": {},
   "source": [
    "# LLM Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61575e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "## pytorch framework\n",
    "import torch # installed by --index-url https://download.pytorch.org/whl/cu118 to GPU availability\n",
    "## AutoModelForCausalLM -> pretrained model\n",
    "## AutoTokenizer -> tokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# Visual\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c764748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will use: GPU.\n"
     ]
    }
   ],
   "source": [
    "print(\"I will use:\", end=\" \")\n",
    "print(\"GPU.\") if torch.cuda.is_available()==True else print(\"CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cfa8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and the model (deepseek)\n",
    "model_name = \"gpt2-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    # trust_remote_code=True, # only for less common modes (always checked before using one of them)\n",
    "    device_map=\"auto\" # auto if GPU; cpu if CPU\n",
    ").to(\"cuda\") # to force if device_map=\"auto\" does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b7d7b0",
   "metadata": {},
   "source": [
    "## Chat loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86007f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start chatting with the bot (type 'quit' to stop)\n",
      "Tell me something\n",
      ">> Bot: \n",
      " Pentagon officials said they did not know whether the drone would be used to attack Islamic State in Syria, but they hoped it would be able to deliver accurate strikes against other militants.\n",
      "\n",
      "U.S. officials told Fox News that they hoped the drone, which has a radar-evading design, would help the U.S. hunt down other terrorists trying to infiltrate into the country through Syria and Iraq.\n",
      "\n",
      "\"We're not in the business of killing people on the ground,\" said one military official, who asked not to be identified.\n",
      "\n",
      "The White House also said that it would allow the drone to take off and fly for at least 20 minutes before being put down. The military official added that the U.S. would not send the drone to a combat zone for any mission.\n",
      "\n",
      "\"We're not going to be in Syria or Iraq with a drone,\" said the military official.\n",
      "\n",
      "White House spokesman Josh Earnest said the White House was still in the process of discussing the drone's capability to kill in Syria. He said the administration was waiting for the Pentagon to finalize its plans to use the drone in Syria before the decision was made.\n",
      "\n",
      "\"We are working closely with our allies to determine what steps will be required to ensure this critical weapon system is deployed as planned,\" Earnest said.\n",
      "\n",
      "He said the White House was working with the Pentagon on the logistics of placing the weapon in the ground, which would ensure its safe delivery to Syria.\n",
      "\n",
      "The Pentagon said it was aware of the news report.\n",
      "\n",
      "\"The U.S. military and our partners have worked closely with their Syrian allies to ensure the safe and secure delivery of the unmanned aerial system to Syria, as required by existing U.S. law,\" the Pentagon said in a statement.\n",
      "\n",
      "The Pentagon said it is working with U.S. partners \"to ensure safe and secure\" the safe and secure delivery of the unmanned aerial system, which is being used to target Islamic State militants who have been advancing in the region. The weapon has been used successfully to target U.S. and allied forces in Iraq and Afghanistan, but has been plagued by errors, according to the Pentagon.\n",
      "\n",
      "\"We are continuing to work closely with our allies to ensure the safe and secure delivery of the unmanned aerial system to Syria, as required by existing U.S. law,\" the Pentagon said.\n",
      "\n",
      "A U.S. defense official, who spoke on condition of anonymity to discuss military matters, said that the weapon would not be dropped from an aircraft unless the U.S. was certain that an attack was imminent in Syria. He said the U.S. is also trying to reduce the risk that a Syrian aircraft could be used to carry the weapon, which could cause it to fire without warning.\n",
      "\n",
      "The official said the U.S. was looking to place the drone in the hands of the Syrian military and was not planning to send it to a combat zone for any mission. The official said the U.S. was also working with Syrian intelligence agencies to ensure the safety of the weapon, but did not elaborate.\n",
      "\n",
      "U.S. officials said they do not expect to use the weapon until the final stages of the strike hunt, which begins next week. A U.S. official said that the U.S. would not be using the weapon in any combat zone, but could use it as an extended strike force if the administration decides it could be useful.\n",
      "\n",
      "The officials spoke on condition of anonymity because they were not authorized to brief reporters on the matter.\n",
      "\n",
      "The officials said that, for now, the U.S. is working with the Syrian military and the Syrian intelligence agencies to ensure the safety of the weapon.\n",
      "\n",
      "The U.S. military and its partners are still working to determine how the U.S. military and other allies will use the weapon in Syria and Iraq, a U.S. defense official said, but the U.S. is \"in the process of working with their Syrian allies to determine how we can safely deploy this weapon.\"\n",
      "\n",
      "The official said the U.S. was also working closely with its allies to ensure the safety of the weapon, but did not elaborate.\n",
      "\n",
      "The official said that the U.S. is also working with Syrian intelligence agencies to ensure the safety of the weapon, but did not elaborate.\n",
      "\n",
      "The official also said that the U.S. is working with U.S. and Israeli intelligence agencies to assess the threat posed by Islamic State militants who have been advancing on cities and towns in Iraq and Syria.\n",
      "\n",
      "Syrian officials have said that Islamic State has used U.S.-manufactured weapons such as the Reaper, the Russian-made Hwasong-12 and the Patriot missile systems to kill civilians, including children.\n",
      "\n",
      "The White House also said the drone has\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "# Chat loop\n",
    "chat_history_ids = None\n",
    "n_turns = 10\n",
    "max_new_tokens = 1000\n",
    "print(\"Start chatting with the bot (type 'quit' to stop)\")\n",
    "\n",
    "for _ in range(n_turns):\n",
    "    # User input\n",
    "    user_input = input(\">> User: \")\n",
    "    print(user_input)\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    # Tokenize the input\n",
    "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "    # Generate the response (from history if exists)\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1).to(\"cuda\") if chat_history_ids is not None else new_input_ids\n",
    "\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids,\n",
    "        max_length=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "\n",
    "    # Decode response\n",
    "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\">> Bot: \\n {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952ee747",
   "metadata": {},
   "source": [
    "## Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "899a78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def respond(message, history):\n",
    "    prompt = message\n",
    "    output = pipe(\n",
    "        prompt, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id, \n",
    "        do_sample=True, \n",
    "        temperature=0.7\n",
    "    )\n",
    "    return output[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "chat_ui = gr.ChatInterface(\n",
    "    fn=respond,\n",
    "    type=\"messages\",\n",
    "    textbox=gr.Textbox(placeholder=\"Ask something...\", scale=7),\n",
    "    title=\"DeepSeek Chatbot\",\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "chat_ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33474ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.close_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
